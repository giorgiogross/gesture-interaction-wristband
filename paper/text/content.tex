%% content.tex
%%

%% ==============
\chapter{Related Work}
\label{ch:Related Work}
%% ==============

Gesture recognition with wearable devices has been an important research topic over the years. 
There have been a lot of works which used accelerometer-based devices, such as a Wii remote or a data glove. 
Xu, Zhou and Li experimented with classifying seven hand gestures using the sign sequence of the gesture acceleration as the main feature (\cite{xu}). 
They tried three different recognition algorithms in order to find the most suitable one. 
The movement data was gathered using MEMS 3-axis accelerometer with a Bluetooth connection to a PC. 
With a dataset of overall 628 samples they achieved an average recognition accuracy of 95.6\% and they concluded, that in this experiment the recognition algorithm based on sign sequence and template matching was the most accurate algorithm. 
Liu also used template matching in order to classify accelerometer data (\cite{liu}). 
In their experiments, they utilized the three-axis accelerometer from the Wii remote as input device for their classifier. 
They evaluated a large gesture library with over 4000 samples for eight gesture patterns and their results show an overall 98.6\% accuracy and a recognition delay of only 300 ms. 
Kim, Thang and Kim used several accelerometers, with which they created a data glove that was connected via Bluetooth with the PC (\cite{kim}). 
They performed simple hand gesture recognition by considering a 10-point moving average of the acceleration with which they could achieve a recognition accuracy of 100\% for only three distinct hand gestures. 

Further work has been done focusing on which machine learning algorithm performs best regarding gesture recognition. 
Belgioioso examined acceleration data collected from an iPod Touch and a HTC-Explorer smartphone. 
They experimented based on a dataset of overall 550 samples for four types of gestures and tested classification approaches such as Support Vector Machine (SVM), Relevance Vector Machine (RVM), Sparse Bayesian Learning (SBL) or k-Nearest Neighbor (kNN) (\cite{belgioioso}). 
For feature extraction, they employed a Principal Component Analysis which is an approach to reduce the dimensionality of a data set consisting of many variables correlated with each other. 
The work concluded that SVM has proved to be the approach with the highest accuracy in recognizing gestures, while SBL based approaches showed comparable classification performances with less computational cost required.
To the best of our knowledge, this work presents the first gesture recognition study using the Thunderboard react.


%% ==============
\chapter{System Design}
\label{ch:SystemDesign}
%% ==============

In order to maximize reusability of code we decided to split the app into the six main components shown below. As you can see, our project consists of two individual parts, the recorder app and the recognizer app, sharing two helper modules, labeled Input and Connector. We implemented a core module for each part which is responsible to coordinate data streams and event responses. As such, the core modules instantiate their helper modules and use their interfaces to communicate with them. It is also worth knowing that some modules are divided into multiple files which are documented directly in our code.

%% ===========================
\section{Components}
\label{ch:SystemDesign:sec:Components}
%% ===========================

\begin{figure}[htp]
\begin{center}
  \includegraphics[width = \textwidth]{Component_diagram_recognizer.jpg}
\caption{Recognizer App}
\end{center}
\end{figure} 

\begin{figure}[htp]
\begin{center}
  \includegraphics[width = \textwidth]{Component_diagram_recorder.jpg}
\caption{Recorder App}
\end{center}
\end{figure} 

\newpage

%% ===========================
\subsection{Connector}
\label{ch:SystemDesign:sec:Components:sub:Connector}
%% ===========================
\setlength\intextsep{0pt}
\begin{wrapfigure}{R}{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{Connector_flow.jpg}
\caption{Establishing the connection to the Thunderboard React}\label{fig:flow_chart}
\end{wrapfigure}

As we ran into several issues trying to connect the Thunderboard React using Python we implemented the connector in JavaScript and used NodeJS libraries for interfacing the bluetooth device. To launch the recorder or recognizer app you need to launch the connector and pass it the path to the desired core module. After launching the connector it performs the steps shown in the flow chart on the right.

\begin{wrapfigure}{R}{0.4\textwidth}
\centering
\includegraphics[width=0.4\textwidth]{recorder_screenshot.png}
\caption{Recorder Tool}\label{fig:recorder_screenshot}
\end{wrapfigure}

%% ===========================
\subsection{Input}
\label{ch:SystemDesign:sec:Components:sub:Input}
%% ===========================

The input module starts a new thread upon instantiation to avoid blocking the calling thread. In its new thread it keeps reading the sensor data from stdin, parses it into floating point values and inserts them into a ring buffer. Its interfaces can be used to set up the buffer, to reset the buffer, to insert new data into the buffer or to start recording.  In the latter case, the module finds out when the actual gesture started on its own by comparing the acceleration vector length to a threshold value. As soon as the buffer fills up its content is written to a static CSV file autonomously. The input module never takes action on its own but is still smart enough to perform a full control flow to record a gesture or to maintain the ring buffer.

%% ===========================
\subsection{Recorder}
\label{ch:SystemDesign:sec:Components:sub:Recorder}
%% ===========================

The recorder module provides a simple user interface which is used to specify the gesture you want to record as well as to explicitly start recording. That is done through a button which has to be pressed before each single gesture; we implemented it this way in order to avoid accidentally record a gesture which would pollute our training set. It also is responsible for instantiation and hooking up the input module.

%% ===========================
\subsection{Scanner}
\label{ch:SystemDesign:sec:Components:sub:Scanner}
%% ===========================

The scanner module is only used by the recognition module. It is used for monitoring the ring buffer content and for checking whether a gesture was observed. Thus, it instantiates, trains and maintains the machine learning algorithm used to classify live sensor data. Training includes transforming the recorded raw acceleration and tilt angle data into features and feeding them to the machine learning algorithm. All live sensor data will  also have to be transformed into the same features so that they can be classified. We describe what features are and how we transformed the raw data in the next chapter.
To avoid unnecessary complexity, we included the cross validation used to evaluate the performance of different machine learning algorithms directly into the scanner module. Cross validation includes leave one out, 10-fold cross validation and 25-fold cross validation and is performed during instantiation.


%% ===========================
\subsection{Recognition}
\label{ch:SystemDesign:sec:Components:sub:Recognition}
%% ===========================

The recognition module used in the recognizer app is responsible to instantiate and hooking up the input module. It also instantiates the dashboard and implements the logic required to notify it about a recognized gesture.

%% ===========================
\chapter{Data Collection \& Manipulation}
\label{ch:DataCollection}
%% ===========================

Working  with  machine  learning  algorithms  requires  certain  actions  in  order  to choose, prepare and classify data streams.
Choosing the right data is the key decision in order to be able to train the  algorithm properly.
In addition to this  step, preparing data and transforming it into “features” is equally important. 

%% ===========================
\section{Data Collection}
\label{ch:DataCollection:sec:DataCollection}
%% ===========================

To  define  which  data  will  be  collected  we  had  to  think  about  which  kind  of gestures  we  will  want  to  recognize.
Attaching  the  sensors  to  the  back  of  our  hand enables us to observe acceleration data  for x,  y, and z axis  as well  as tilt  angles  alpha, beta and gamma.
We thought about attaching a magnet to our thumb so that we can use the hall-sensor as well,  but we decided to leave this  for  further research.
The collected acceleration and tilt angle data can later be used to distinguish gestures from each other.
For collecting all the data we need, we use our recorder system, which reads data from the Thunderboard by using our connector module and giving us the possibility to tag the recorded gesture with an ID.
The Thunderboard is therefor attached to the back of the right hand.
To avoid recording and scanning data observed during normal gesticulation we require the acceleration vector length of each gesture to exceed a certain threshold.
In our code we set this treshold to 1.2 G.
As soon as the treshold is exceeded, our recorder saves the next four data points and the specified gesture ID as row in a CSV file.
All in all, our team recorded 120 data sets per gesture, so 480 data sets overall.
Because the recorded data is really slow to progress we decided to stop at this point, although we initially wanted external people to record some data for us.

%% ===========================
\section{Gestures}
\label{ch:DataCollection:sec:Gestures}
%% ===========================

Really early in development, we thought about what gestures we can detect and what gestures are useful for us.
We came up with many ideas, but the first two gestures we implemented were the two most intuitive gestures for us: Swipe Left and Swipe Right.
For easy recognition of the gesture, we decided to put rules on the execution of these gestures.
A \glqq swipe left\grqq{} gesture consists of a very fast movement of the active hand to the left, followed by a slow movement back to the initial position.
Both movements should not include a big tilt of the board.
The \glqq swipe right\grqq{} gesture is just the same, but with switched directions.
By adding those rules we increased the success of recognition hugely.
As a consequence, we removed the first data sets we recorded, without those rules, from the CSV.
After a few weeks we reached a great success rate on the left and right swipes, so we added additional swipes \glqq Up\grqq{} and \glqq Down\grqq{} and a punch gesture.
The punch gesture is executed like a normal punch with the fist.
Since the \glqq Swipe Down\grqq{} and the punch were often confound, we decided to exclude the \glqq Swipe Down\grqq -Gesture from our work.

%% ===========================
\section{Feature Design}
\label{ch:DataCollection:sec:FeatureDesign}
%% ===========================

Before we can train our algorithm, we  have to transform the recorded data into a set  of  features.
Choosing  and  designing  those  features  affects  the  performance  of  the gesture recognition algorithm heavily.

%% ===========================
\subsection{Preprocessing}
\label{ch:DataCollection:sec:FeatureDesign:sub:Preprocessing}
%% ===========================

Standardization,     scaling,     normalization     and     binarization     are     common preprocessing techniques.
Though, as the acceleration vector and tilt angle sizes are key characteristics  to  distinguish  gestures  scaling,  normalization  and  binarization  are  not applicable for our use case. 

%% ===========================
\subsection{Features}
\label{ch:DataCollection:sec:FeatureDesign:sub:Features}
%% ===========================

\begin{figure}
\begin{center}
  \includegraphics[width = 0.88\textwidth]{data.png}
\caption{Choosing our features by looking at exemplary data points}\label{fig:datamining}
\end{center}
\end{figure} 

We  first  thought  about  using  the  variance  as  a  feature,  but  early  tests  have already revealed  that  the  acceleration  data  often  has  the same variance across all of our tested gestures. We needed to find more useful features, that have different values when we execute different gestures.

The way we thought about this set of features was very intuitive. First, we recorded some gestures that we wanted to implement.
We then created graphs for each of our data sets and looked what is characteristic for them.
In Figure \ref{fig:datamining} for example, we could see that the Swipe Left (Swipe Right) gesture first has a minimum (maximum) on the X-Axis, then a maximum (minimum).
So we decided to use maximum  and  minimum  acceleration  per  axis, because they  differ  widely  across gestures  and  are  worth  adding  to  our  feature  set.

To give our classifier an idea of which minimum/maximum was the first we put them in the right order. So, if we have a maximum first this will be the first feature and the minimum will become the second feature.
For  instance,  swiping  left  first  shows  a negative  minimum  acceleration, e.g. -1.7G, and then a positive  maximum acceleration, e.g. 1.2G, on the x axis.
We now pick those values out of our buffer and figure out which one was earlier, so we can put them in our feature vector in the right order.
In  this example, we end up with the tuple (-1.7, 1.2).

To include the tilt angles into our feature set, we had to come up with a different approach.
In contrast to the acceleration, not only the variance of the given data was a bad idea for either angles, but also their minimum and maximum values are often equally great across different gestures.
Anyway, taking the minimum and maximum into our feature set increased our precision a little bit, so we continued to use them.

Summing  up  all  angles  led  to non-satisfying  results because they mostly sum up to a value near zero,  so  we  kept thinking about  more  features  per  angle:
One  sums  up  the absolute  values  of  negative  angles  and  the  other  one  sums  up  the  absolute values  of  the  positive  angles.

Because the Thunderboard does a permanent recalibration the absolute angles were not really useful. So, in the next step we decided to use only the differences between two angles in our buffer.
This new approach improved our gesture recognition by about 5\%, as you can see in picture \ref{fig:angle_feature}.

\begin{figure}[htp]
\begin{center}
  \includegraphics[width=0.5\textwidth]{angle_feature.png}
\caption{Comparison of our old and our new idea}\label{fig:angle_feature}
\end{center}
\end{figure}

\newpage 

%% ===========================
\section{Data Classification}
\label{ch:DataCollection:sec:DataClassification}
%% ===========================

Depending on the underlying training set, each classifier performs a bit different. We tested the K-Neighbors classifier with five and eleven nearest neighbors, Random Forest classifier with 10 and 100 estimators,  Decision Tree classifier, Support Vector Machine and Gaussian Bayes, which is a neural network. We then created a Voting classifier based on two of the best classifiers to push the limits even further. Each classifier was evaluated with leave one out and 10-fold cross validation techniques, the results are summarized in figure \ref{fig:10fold} below:
\newline
\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
    title = {},
    width=15cm,
    height=9cm,
    xtick=\empty,
    ytick={0,5,...,100},
    ylabel=Value in \%,
    extra x ticks={1,...,8},
    extra x tick labels={%
		Random Forest with 10 estimators,
        Random Forest with 100 estimators,
        Decision Tree,
        5-Neighbors,
        11-Neighbors,
        SVM,
        Gaussian Bayes,
        Voting    
    },
    extra x tick style={
           grid=none,
           tick label style={rotate=90}
           },
    ybar
    ]

\addplot[
    fill=blue!60,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
] 
table [y error=error] {
x   y           error    label
1   92   12 1
2   95    10 2
3   89   22 3
4   92   13 4 
5   89    16 5 
6   96   5 6
7   64    30 7 
8   95   9 8
};

\addplot[
    fill=red!60,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
] 
table [y error=error] {
x   y           error    label
1   91   16 1
2   94    10 2
3   88   24 3
4   92   13 4 
5   89    16 5 
6   96   5 6
7   64    30 7 
8   95   8 8
};

\addplot[
    fill=green!60,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
] 
table [y error=error] {
x   y           error    label
1   91   12 1
2   94    12 2
3   88   24 3
4   92   13 4 
5   89    16 5 
6   96   5 6
7   64    30 7 
8   95   8 8
};

\legend{Precision,Accuracy,Recall}
\draw ({rel axis cs:0,0}|-{axis cs:0,0}) -- ({rel axis cs:1,0}|-{axis cs:0,0});
\end{axis}
\end{tikzpicture}
\caption{10-fold cross validation}\label{fig:10fold}
\end{figure}

As you can see, the Gaussian Bayes performs poorly compared to the other classifiers. Its precision, accuracy and recall values as well as its standard deviation are unfavourable for practial use. In contrast, the Decision Tree classifier reaches almost 90\% precision, accuracy and recall value. The prediction is based on a tree; inputs always iterate through each node until they reach a leave. Each node checks and classifies the input values and has one incoming and two outgoing edges. The outgoing edges connect to either another classifier node or to a leave. The leaves finally state which gesture the input data corresponds to. This means that our classifier will always predict a gesture with either 100\% or 0\% probability, there is nothing in between. Using a tree is a promising solution, but due to the mentioned behaviour poor predictions cannot be filtered. For instance, knowing the real probability of the prediction allows us to check if it exceeds a certain threshold. We set that threshold to 72\% for other classifiers, so predictions below 72\% probability were ignored. However, combining multiple trees by using a so called Random Forest led to much better results and allows us to monitor the prediction probability. As shown in figure \ref{fig:10fold}, our classifier with 100 estimators reaches 95\% precision and 94\% accuracy and recall. 
\newline
\begin{figure}[H]
\begin{tikzpicture}
\begin{axis}[
    title = {},
    width=15cm,
	ymin=0,
    height=9cm,
    xtick=\empty,
    ytick={0,5,...,100},
    extra x ticks={1,...,8},
    extra x tick labels={%
		Random Forest with 10 estimators,
        Random Forest with 100 estimators,
        Decision Tree,
        5-Neighbors,
        11-Neighbors,
        SVM,
        Gaussian Bayes,
        Voting    
    },
    extra x tick style={
           grid=none,
           tick label style={rotate=90} % <-- this is added
           },
    legend style={fill=none},
    ybar
    ]

\addplot[
    fill=blue!60,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt}
] 
table [y] {
x   y      label
1   93.5   1
2   96   2
3   91   3
4   93.5   4 
5   93.1   5 
6   96.9   6
7   63.5   7 
8   96.5   8
};

\addplot[
    fill=orange!60,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt}
] 
table [y] {
x   y      label
1   6.3   1
2   59.8   2
3   0.1   3
4   1.1   4 
5   1.3   5 
6   0.2   6
7   0.3   7 
8   62.9   8
};

\legend{Accuracy in \%,Prediction Time in ms}
\draw ({rel axis cs:0,0}|-{axis cs:0,0}) -- ({rel axis cs:1,0}|-{axis cs:0,0});
\end{axis}
\end{tikzpicture}
\caption{Leave one out}\label{fig:loo}
\end{figure}

The K-Neighbors classifier follows a different approach. Simply put, training data is mapped into a coordinate system. Input data is then mapped the same way and classified by checking the K nearest training data points in that coordinate system relative to the input data. Whatever gesture the majority of these K neighbors is assigned to is the predicted gesture of our input. In our case K was five or eleven. Considering just 5 neighbors showed off to be the better solution and compares very vell to our Random Forest classifier with 100 estimators: The K-Neighbors classifier performs about 2-3\% worse and has a higher standard deviation equal to 13\% in precision, accuracy and recall instead of 10\% - 12\%.

Having examined those common classifiers we combined the Random Forest classifier with 100 estimators and 5-Neighbors classifier as estimators into a Voting classifier. The Voting classifier passes input data to all its estimators and makes its decision based on their averaged predicted gesture probabilites. Tests revealed that giving the Random Forest predictions twice the weight of the 5-Neighbors predictions while using soft voting increases the accuracy and recall of our Voting classifier to 95\% while slightly decreasing the standard deviation to 9-8\%. The weight determines in the case of soft voting, as we applied it, the weight of the gesture probabilities before averaging.

Yet, the Support Vector Machine (SVM) classifier still beats the Voting classifier: 96\% accuracy, precision and recall with a standard deviation of 5\% each are the main reasons why we would chose the SVM classifier for dashboard interaction. Since well-tuned features and improved data preprocessing are important for an efficient SVM classifier we were only able to reach these results in the final phase of our work. For that reason the SVM classifier may be examined in further research and is currently not used in our dashboard application.

In order to provide a convenient user experience, gestures need to be predicted fast enough during app usage. We measured the average prediction time for each classifier during our leave one out tests. As our data set consists of 480 samples we iterated 480 times over it and left out a different data set on each iteration. The left out data set was used for testing, the remaining 479 data sets were used for training. The results are presented in figure \ref{fig:loo}.

Although the Voting classifiers prediction time of 62 milliseconds is the highest one it competes very vell against the results of \cite{liu}; our achieved delay is about five times shorter and therefore proves itself appropriate for real world applications. We implemented the Voting classifier in the dashboard and got statisfying test results. One might notice that the 5-Neighbors classifier needs approximately one millisecond and still performs almost as well as the Random Forest classifier with 100 estimators. So the relatively large prediction time of the Voting classifier is mainly caused by the Random Forest classifier we applied to the classifier set. Overall, the longer prediction time pays off as an increase of 3\% in accuracy, precision and recall values. 
In contrast, the SVM classifier stands out with a prediction time of 0.2ms. As it also performs slightly better than the Voting classifier it is the option to choose for production use.


%% ==============
\chapter{Dashboard}
\label{ch:Dashboard}
%% ==============

The main goal of our seminar work was to implement a gesture recognition program with which we can reliably control applications like dashboards on the computer.
Therefore, we created a test dashboard with which we examined the usability of our program.
Just like the previously decribed scanner, the dashboard module is only used by the recognizer app.
It provides the user with a user interface showing charts and diagrams based on stock market data which can be modified by steering elements like a dropdown menu and radio buttons.
The main characteristic of the dashboard is that it can be controlled only with a few simple buttons.
This makes the steering with our gesture recognition program easier since we can use the python package “pyautogui” to translate recognized gestures into commands on our user interface.

We decided to use the swipe left and right gestures to select a value in a dropdown menu and we use a punch gesture to cycle through the available years in data since these commands felt relatively intuitive. 

Of course, this is just a simplified example of a dashboard with only a few features compared to what can be found in actual corporate dashboards which allow more sophisticated commands.
Nevertheless, our dashboard can be easily extended with further features by implementing more steering elements like slide bars or check buttons and by adding more recognizable gestures with which these elements can be controlled.  

\begin{figure}[htp]
\begin{center}
  \includegraphics[width = \textwidth]{dashboard.png}
\caption{Our demo application front end}
\end{center}
\end{figure} 


%% ===========================
\chapter{Testing}
\label{ch:Tests}
%% ===========================

%% ===========================
\section{Functionality}
\label{ch:Results:sec:Functionality}
%% ===========================

In our tests we noticed that we can not fully reach the 95\% of precision that we theoretically achieved in our calculations.

The key factor why we did not reach this precision is the very low frequency of data given to us by the Thunderboard.
We recieved only 5 data samples per seconds while our gestures are very quickly executed, resulting in only 3 to 4 samples per gesture.
This leads to the problem, that the movement to the left and the movement back to the right can be condensed to one data point.
In this case we only get one data point from two different movements.

We tried to improve this by recording more samples in different speeds and with slightly different movement, but the more data we used the slowlier the algorithms got.

In the end we are very happy that we recognized 43 of 50 gestures which makes a total of 86\% in our final test.
Those gestures were then correctly converted into dashboard commands.

%% ===========================
\section{Usability}
\label{ch:Results:sec:Usability}
%% ===========================

In our usability test we asked five people that had no contact to our development before what they think about it.
We gave them our Thunderboard glove and let them test the dashboard after a very quick introduction what gestures we support and what they do.
The following section sums up the answers of our testers:

After difficulties in the beginning and a recalibration process, the program worked well for our first tester, some gestures better than others.
Three other testers had no difficulties in using the dashboard, only a few gestures were not recognized because of too slow movement.
The last tester had bigger problems in getting their gestures to do the right thing - We think the board was not calibrated correctly.

We next asked the testers if they think we chose good gestures for our work.
They all said the gestures were well chosen except the punch gesture needs too much power in order to be recognized.

Afterwards we asked if the method of controlling the dashboard with gestures is easy to learn.
Our testers stated that there is a short period of learning in the beginning until you know how to perform the gestures and you get enough power to activate our algorithm.
After that, it's natural and easy to control the dashboard.

Overall, most testers said that the gesture recognition is not very suitable for controlling a dashboard but rather for a presentation of a dashboard in a conference.
Here you have the advantage that you do not need to do several clicks for each action.
Instead you can control the dashboard while presenting just with your arm movement.
Another negative point that was mentioned multiple times is that the gestures should be way more subtle, since you have to do fast and long movements so that the tool can detect the right gesture.
We think that with a higher sampling rate of the Thunderboard, we could achieve much better results with more subtle gestures.

%% ===========================
\chapter{Results}
\label{ch:Results}
%% ===========================

In this seminar paper, we present a gesture recognition system for dashboard steering via a Thunderboard React. 
Core of this implementation are the machine learning algorithms which are used to classify the input data and to recognize the different gestures. 
We used several algorithms, like Random Forest, K-Neighbors, Support Vector Machine or a Voting classifier which we trained with overall 480 samples for four different gestures. 
Our experiments demonstrated that our implementation achieves an expected accuracy of 95\% with a delay of only 62 milliseconds by using a Voting classifier. 
Furthermore, we used the gesture recognition for controlling a dashboard that we created. 
In practice, over 86\% of the gestures could be recognized and translated correctly into dashboard commands, which is a decent result. 

We also investigated the usability of the gesture based dashboard steering. 
In experiments, we let participants use the gesture recognition for cotrolling the dashboard and evaluated their experience as shown in chapter \ref{ch:Results:sec:Usability}.
Most testers liked our approach, but said the gestures are too rough and must be executed too fast.
Our proposal for a solution is improving the sampling rate of our given hardware to detect more subtle movement in the short time of a gesture.
The second most important statement of our testers was that the tool is not very intuitive for controlling a dashboard but maybe applicable to hold a presentation with.

Nevertheless, our work exposes several extension opportunities. 
First of all, further recognizable gestures can be added in order to increase the usability of our implementation for industrial purposes. 
This would also allow the user to control a more sophisticated dashboard with more steering elements than the one used in our work.
Further gestures and a higher accuracy can be realized by utilizing data from the Hall sensor on the Thunderboard react, so that more features can be calculated as training data for the machine learning algorithm. 
Also, more sample data for the training of the algorithms is valuable, which was unfortunately not possible in this work due to the limited time that was available. 
Moreover, a combination of our work with tactile feedback devices can also be implemented in order to enhance a user’s dashboard experience even more. 


