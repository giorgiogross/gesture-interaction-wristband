%% introduction.tex
%%

%% ==============================
\chapter{Introduction}
\label{ch:Introduction}
%% ==============================

Nowadays gesture recognition has achieved a high usage and meaning, especially for medical purposes and industrial production processes.
These new technologies are available not only for industrial purposes, but also for the mass market, for which devices like the Xbox Kinect or the Wii Remote set the basis a few years ago.
Until now, many research groups (\cite{vision1},\cite{vision3},\cite{vision2}) have already worked on technologies which enable the user to control computers with gestures.
Most of these devices work vision-based, i.e. with cameras which recognize the visual changes caused by body movement and translate them into computer commands.
These applications have the disadvantage that the user cannot move freely in the room, since he is bounded to the space captured by the camera in order to transmit the commands properly.
This can be impractical for specific situations, e.g. presentations, when flexibility is needed considering the movement in the room.

Furthermore, there are also several technologies which track the physical data of the movement with the help of sensors (accelerometer, gyroscope, etc.) to recognize a gesture.
These can be used independently from the location of the user in the room, but often demands expensive equipment, for instance a data glove.
In order to develop a cheap and accurate sensor based gesture recognition device, different approaches use data from an accelerometer to classify gestures.
These show a high accuracy, but are limited in the number of different identifiable gestures due to the less data.

In our work, we used the “Thunderboard React”, which is a development platform with several sensors including an accelerometer and a gyroscope.
We examined, how well such a sensor platform is suited for gesture recognition and how it enhances dashboard user experiences.
We did this by calculating “features” like the maximal acceleration from the collected data and classified these with different machine learning approaches.
Overall, we established to recognize 4 different gestures with an expectancy accuracy of 95\%.
Furthermore, we created a dashboard to test the usability of our implementation.
